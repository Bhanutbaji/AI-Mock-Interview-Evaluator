{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":639622,"sourceType":"datasetVersion","datasetId":316368},{"sourceId":653195,"sourceType":"datasetVersion","datasetId":325566}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":291.936497,"end_time":"2025-03-21T09:14:30.966507","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-03-21T09:09:39.03001","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##                               Speech Emotion Recognition","metadata":{"id":"OYjabkTtQnPm","papermill":{"duration":0.008589,"end_time":"2025-03-21T09:09:41.745208","exception":false,"start_time":"2025-03-21T09:09:41.736619","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense, Dropout, Bidirectional, LSTM, Attention\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport IPython.display as ipd","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-03-21T09:09:41.76203Z","iopub.status.busy":"2025-03-21T09:09:41.761685Z","iopub.status.idle":"2025-03-21T09:09:56.020454Z","shell.execute_reply":"2025-03-21T09:09:56.019791Z"},"id":"N70r4J8GQnPn","papermill":{"duration":14.268974,"end_time":"2025-03-21T09:09:56.022099","exception":false,"start_time":"2025-03-21T09:09:41.753125","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import librosa","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:09:56.038799Z","iopub.status.busy":"2025-03-21T09:09:56.038356Z","iopub.status.idle":"2025-03-21T09:09:56.051981Z","shell.execute_reply":"2025-03-21T09:09:56.051392Z"},"papermill":{"duration":0.023158,"end_time":"2025-03-21T09:09:56.053277","exception":false,"start_time":"2025-03-21T09:09:56.030119","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install librosa audiomentations ","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:09:56.0694Z","iopub.status.busy":"2025-03-21T09:09:56.069198Z","iopub.status.idle":"2025-03-21T09:10:01.743054Z","shell.execute_reply":"2025-03-21T09:10:01.742034Z"},"papermill":{"duration":5.683458,"end_time":"2025-03-21T09:10:01.744482","exception":false,"start_time":"2025-03-21T09:09:56.061024","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install soundfile","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:10:01.7636Z","iopub.status.busy":"2025-03-21T09:10:01.763329Z","iopub.status.idle":"2025-03-21T09:10:05.137258Z","shell.execute_reply":"2025-03-21T09:10:05.136266Z"},"papermill":{"duration":3.385001,"end_time":"2025-03-21T09:10:05.138785","exception":false,"start_time":"2025-03-21T09:10:01.753784","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import audiomentations as A\nimport soundfile as sf\nfrom audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift\n","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:10:05.157969Z","iopub.status.busy":"2025-03-21T09:10:05.157652Z","iopub.status.idle":"2025-03-21T09:10:05.337584Z","shell.execute_reply":"2025-03-21T09:10:05.336878Z"},"papermill":{"duration":0.191275,"end_time":"2025-03-21T09:10:05.339302","exception":false,"start_time":"2025-03-21T09:10:05.148027","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:10:05.360122Z","iopub.status.busy":"2025-03-21T09:10:05.359264Z","iopub.status.idle":"2025-03-21T09:10:05.780446Z","shell.execute_reply":"2025-03-21T09:10:05.779531Z"},"papermill":{"duration":0.433064,"end_time":"2025-03-21T09:10:05.782104","exception":false,"start_time":"2025-03-21T09:10:05.34904","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Using TESS dataframes","metadata":{"id":"6JK1QezKQnPt","papermill":{"duration":0.008641,"end_time":"2025-03-21T09:10:05.80024","exception":false,"start_time":"2025-03-21T09:10:05.791599","status":"completed"},"tags":[]}},{"cell_type":"code","source":"## path to TESS data\ntess_path = '/kaggle/input/toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data'","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:10:05.819104Z","iopub.status.busy":"2025-03-21T09:10:05.818575Z","iopub.status.idle":"2025-03-21T09:10:05.822051Z","shell.execute_reply":"2025-03-21T09:10:05.821235Z"},"papermill":{"duration":0.014159,"end_time":"2025-03-21T09:10:05.823253","exception":false,"start_time":"2025-03-21T09:10:05.809094","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tess_file_paths = []\ntess_labels = []\n\nfor folder in os.listdir(tess_path):\n    folder_path = os.path.join(tess_path, folder)\n    label = folder[4:] ## folder = OAF_Fear (take from index 4 to last)\n    label = label.lower() ## converting the string to lower\n\n    for file in os.listdir(folder_path):\n        file_path = os.path.join(folder_path, file)\n        ## storing this file path in list\n        tess_file_paths.append(file_path)\n        ## storing the corresponding emotion\n        tess_labels.append(label)","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:10:05.841447Z","iopub.status.busy":"2025-03-21T09:10:05.841215Z","iopub.status.idle":"2025-03-21T09:10:05.972776Z","shell.execute_reply":"2025-03-21T09:10:05.971883Z"},"papermill":{"duration":0.142325,"end_time":"2025-03-21T09:10:05.974361","exception":false,"start_time":"2025-03-21T09:10:05.832036","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## lets make a pandas dataframe for tess data also\ntess_data = pd.DataFrame({\n    'paths' : tess_file_paths,\n    'emotions' : tess_labels\n})\n\n## lets print first 5 rows of this dataset\ntess_data.head()","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:10:05.993074Z","iopub.status.busy":"2025-03-21T09:10:05.992787Z","iopub.status.idle":"2025-03-21T09:10:06.016624Z","shell.execute_reply":"2025-03-21T09:10:06.015956Z"},"papermill":{"duration":0.034462,"end_time":"2025-03-21T09:10:06.01798","exception":false,"start_time":"2025-03-21T09:10:05.983518","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Define augmentations\naugment = A.Compose([\n    A.AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n    A.PitchShift(min_semitones=-2, max_semitones=2, p=0.5),\n    A.TimeStretch(min_rate=0.8, max_rate=1.2, p=0.5)\n])\n\n\n# Create directory for augmented files\naugmented_dir = \"augmented_audio\"\nos.makedirs(augmented_dir, exist_ok=True)\n\n# Process and augment data\naugmented_file_paths = []\naugmented_labels = []\n\nfor file_path, label in zip(tess_file_paths, tess_labels):\n    # Load audio file\n    audio, sr = librosa.load(file_path, sr=None)\n\n    # Apply augmentation\n    augmented_audio = augment(audio, sample_rate=sr)\n\n    # Save augmented audio\n    augmented_file_path = os.path.join(augmented_dir, os.path.basename(file_path).replace('.wav', '_augmented.wav'))\n    try:\n        sf.write(augmented_file_path, augmented_audio, sr)\n        augmented_file_paths.append(augmented_file_path)\n        augmented_labels.append(label)\n    except Exception as e:\n        print(f\"Error writing {augmented_file_path}: {e}\")\n","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:10:06.036719Z","iopub.status.busy":"2025-03-21T09:10:06.036489Z","iopub.status.idle":"2025-03-21T09:11:42.792593Z","shell.execute_reply":"2025-03-21T09:11:42.7919Z"},"papermill":{"duration":96.767255,"end_time":"2025-03-21T09:11:42.794339","exception":false,"start_time":"2025-03-21T09:10:06.027084","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Combine Augmented and TESS data","metadata":{"papermill":{"duration":0.008588,"end_time":"2025-03-21T09:11:42.812515","exception":false,"start_time":"2025-03-21T09:11:42.803927","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Combine Augmented and TESS data\nall_file_paths =  tess_file_paths + augmented_file_paths\nall_labels =  tess_labels + augmented_labels\n\n# Create a dataframe combining both datasets\nemotion_data = pd.DataFrame({\n    'paths': all_file_paths,\n    'emotions': all_labels\n})\n\n# Display the first 5 rows of the combined dataframe\nprint(emotion_data.head())\n","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:11:42.830995Z","iopub.status.busy":"2025-03-21T09:11:42.830714Z","iopub.status.idle":"2025-03-21T09:11:42.837806Z","shell.execute_reply":"2025-03-21T09:11:42.836846Z"},"papermill":{"duration":0.017985,"end_time":"2025-03-21T09:11:42.839086","exception":false,"start_time":"2025-03-21T09:11:42.821101","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_mapping = {\n    'disguist': 'disgust', \n    'fear': 'fearful', \n    'pleasant_surprised': 'pleasant_surprise'\n}","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:11:42.857789Z","iopub.status.busy":"2025-03-21T09:11:42.85755Z","iopub.status.idle":"2025-03-21T09:11:42.8604Z","shell.execute_reply":"2025-03-21T09:11:42.859827Z"},"papermill":{"duration":0.013282,"end_time":"2025-03-21T09:11:42.8616","exception":false,"start_time":"2025-03-21T09:11:42.848318","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion_data['emotions'] = emotion_data['emotions'].replace(label_mapping)\n","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:11:42.87985Z","iopub.status.busy":"2025-03-21T09:11:42.879584Z","iopub.status.idle":"2025-03-21T09:11:42.885113Z","shell.execute_reply":"2025-03-21T09:11:42.884272Z"},"papermill":{"duration":0.015775,"end_time":"2025-03-21T09:11:42.886322","exception":false,"start_time":"2025-03-21T09:11:42.870547","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(emotion_data['emotions'].unique())  # Should show the corrected list of unique emotions\n","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:11:42.904599Z","iopub.status.busy":"2025-03-21T09:11:42.904373Z","iopub.status.idle":"2025-03-21T09:11:42.911355Z","shell.execute_reply":"2025-03-21T09:11:42.910689Z"},"papermill":{"duration":0.017225,"end_time":"2025-03-21T09:11:42.912401","exception":false,"start_time":"2025-03-21T09:11:42.895176","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nplt.figure(figsize=(10,5))\nsns.countplot(x=emotion_data['emotions'], order=emotion_data['emotions'].value_counts().index)\nplt.xticks(rotation=45)\nplt.title(\"Updated Emotion Counts\")\nplt.show()\n","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:11:42.930631Z","iopub.status.busy":"2025-03-21T09:11:42.930409Z","iopub.status.idle":"2025-03-21T09:11:43.20308Z","shell.execute_reply":"2025-03-21T09:11:43.202302Z"},"papermill":{"duration":0.283185,"end_time":"2025-03-21T09:11:43.204431","exception":false,"start_time":"2025-03-21T09:11:42.921246","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to create waveplot\ndef create_waveplot(data, sr, e):\n    plt.figure(figsize=(10, 3))\n    plt.title('Waveplot for audio with {} emotion'.format(e), size=15)\n    librosa.display.waveshow(data, sr=sr)\n    plt.show()\n\n# Function to create spectrogram\ndef create_spectrogram(data, sr, e):\n    plt.figure(figsize=(10, 3))\n    plt.title('Spectrogram for audio with {} emotion'.format(e), size=15)\n    D = librosa.amplitude_to_db(librosa.stft(data), ref=np.max)\n    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log')\n    plt.colorbar(format='%+2.0f dB')\n    plt.show()\n\n# Set emotion and path\nemotion = 'disgust'\npath = np.array(emotion_data.paths[emotion_data.emotions == emotion])[1]  # Ensure data_path has 'Path' and 'Emotions' columns\n\n# Load audio data\ndata, sampling_rate = librosa.load(path)\n\n# Generate waveplot and spectrogram\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\n\nipd.Audio(data, rate=sampling_rate)","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:11:43.224532Z","iopub.status.busy":"2025-03-21T09:11:43.224167Z","iopub.status.idle":"2025-03-21T09:11:44.020287Z","shell.execute_reply":"2025-03-21T09:11:44.019443Z"},"papermill":{"duration":0.808027,"end_time":"2025-03-21T09:11:44.022342","exception":false,"start_time":"2025-03-21T09:11:43.214315","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion='angry'\npath = np.array(emotion_data.paths[emotion_data.emotions==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nipd.Audio(data, rate=sampling_rate)","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:11:44.052366Z","iopub.status.busy":"2025-03-21T09:11:44.052127Z","iopub.status.idle":"2025-03-21T09:11:44.716051Z","shell.execute_reply":"2025-03-21T09:11:44.715229Z"},"papermill":{"duration":0.680301,"end_time":"2025-03-21T09:11:44.717688","exception":false,"start_time":"2025-03-21T09:11:44.037387","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion='fearful'\npath = np.array(emotion_data.paths[emotion_data.emotions==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\n\nipd.Audio(data, rate=sampling_rate)","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:11:44.759918Z","iopub.status.busy":"2025-03-21T09:11:44.759541Z","iopub.status.idle":"2025-03-21T09:11:45.45079Z","shell.execute_reply":"2025-03-21T09:11:45.450062Z"},"papermill":{"duration":0.714261,"end_time":"2025-03-21T09:11:45.4527","exception":false,"start_time":"2025-03-21T09:11:44.738439","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion='sad'\npath = np.array(emotion_data.paths[emotion_data.emotions==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\n\nipd.Audio(data, rate=sampling_rate)","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:11:45.500226Z","iopub.status.busy":"2025-03-21T09:11:45.499949Z","iopub.status.idle":"2025-03-21T09:11:46.183642Z","shell.execute_reply":"2025-03-21T09:11:46.182804Z"},"papermill":{"duration":0.708099,"end_time":"2025-03-21T09:11:46.184998","exception":false,"start_time":"2025-03-21T09:11:45.476899","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion='happy'\npath = np.array(emotion_data.paths[emotion_data.emotions==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\n\nipd.Audio(data, rate=sampling_rate)","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:11:46.243065Z","iopub.status.busy":"2025-03-21T09:11:46.242796Z","iopub.status.idle":"2025-03-21T09:11:46.945789Z","shell.execute_reply":"2025-03-21T09:11:46.944955Z"},"papermill":{"duration":0.733014,"end_time":"2025-03-21T09:11:46.947176","exception":false,"start_time":"2025-03-21T09:11:46.214162","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion='neutral'\npath = np.array(emotion_data.paths[emotion_data.emotions==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\n\nipd.Audio(data, rate=sampling_rate)","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:11:47.013181Z","iopub.status.busy":"2025-03-21T09:11:47.012895Z","iopub.status.idle":"2025-03-21T09:11:47.688092Z","shell.execute_reply":"2025-03-21T09:11:47.687283Z"},"papermill":{"duration":0.709477,"end_time":"2025-03-21T09:11:47.690074","exception":false,"start_time":"2025-03-21T09:11:46.980597","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion='pleasant_surprise'\npath = np.array(emotion_data.paths[emotion_data.emotions==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\n\nipd.Audio(data, rate=sampling_rate)","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:11:47.769485Z","iopub.status.busy":"2025-03-21T09:11:47.769036Z","iopub.status.idle":"2025-03-21T09:11:48.475818Z","shell.execute_reply":"2025-03-21T09:11:48.474828Z"},"papermill":{"duration":0.746277,"end_time":"2025-03-21T09:11:48.477923","exception":false,"start_time":"2025-03-21T09:11:47.731646","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## So we'll try to extract MEL and MFCC features\nmel_features = []\nmfcc_features = []\n\n## traversing all the paths in combined dataset\nfor i in range(len(emotion_data)):\n    ## loading the audio file\n    data, sample_rate = librosa.load(emotion_data.loc[i, 'paths'])\n    ## extracting MEL features\n    mel_features.append(np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis= 0))\n    ## extracting MFCC features\n    mfcc_features.append(np.mean(librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc= 40).T, axis=0))","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:11:48.572759Z","iopub.status.busy":"2025-03-21T09:11:48.572435Z","iopub.status.idle":"2025-03-21T09:13:08.084961Z","shell.execute_reply":"2025-03-21T09:13:08.083972Z"},"id":"AFYo4JN2QnPu","papermill":{"duration":79.558758,"end_time":"2025-03-21T09:13:08.086615","exception":false,"start_time":"2025-03-21T09:11:48.527857","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## lets convert both mel_features and mfcc_features list to arrays\nmel_features_array = np.array(mel_features)\nmfcc_features_array = np.array(mfcc_features)\n\n## lets check shape of both\nprint(f\"Shape of MEL features : {mel_features_array.shape}\")\nprint(f\"Shape of MFCC features : {mfcc_features_array.shape}\")","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:13:08.1688Z","iopub.status.busy":"2025-03-21T09:13:08.168158Z","iopub.status.idle":"2025-03-21T09:13:08.179712Z","shell.execute_reply":"2025-03-21T09:13:08.178948Z"},"id":"N6riJtuqQnPv","outputId":"fb3f0c2f-7598-472b-a084-ad418a75bd5e","papermill":{"duration":0.0532,"end_time":"2025-03-21T09:13:08.181058","exception":false,"start_time":"2025-03-21T09:13:08.127858","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features = np.hstack((mel_features_array, mfcc_features_array))\n\n## now lets check shape\nprint(f\"Shape of feature data : {features.shape}\")","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:13:08.262005Z","iopub.status.busy":"2025-03-21T09:13:08.261708Z","iopub.status.idle":"2025-03-21T09:13:08.266387Z","shell.execute_reply":"2025-03-21T09:13:08.265729Z"},"id":"5wNq6i5tQnPv","outputId":"98650413-4338-4abb-d35c-7b81a8eebf09","papermill":{"duration":0.046638,"end_time":"2025-03-21T09:13:08.267496","exception":false,"start_time":"2025-03-21T09:13:08.220858","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## lets make a datagframe containing these features and corresponding emotions\ndf_features = pd.DataFrame(features)\n## combine both emotion_data and df_features\ndata = pd.concat([emotion_data, df_features], axis=1)\n## lets print first 5 rows of new data\ndata.head()","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:13:08.347718Z","iopub.status.busy":"2025-03-21T09:13:08.347381Z","iopub.status.idle":"2025-03-21T09:13:08.370866Z","shell.execute_reply":"2025-03-21T09:13:08.370177Z"},"id":"bR5MXsNZQnPv","outputId":"ff92f878-0d6b-4e77-eddf-136200b36ec5","papermill":{"duration":0.065011,"end_time":"2025-03-21T09:13:08.372028","exception":false,"start_time":"2025-03-21T09:13:08.307017","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## we don't need paths column we can drop it\ndata_new = data.drop(columns=['paths'])\ndata_new.head()","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:13:08.453666Z","iopub.status.busy":"2025-03-21T09:13:08.453352Z","iopub.status.idle":"2025-03-21T09:13:08.472382Z","shell.execute_reply":"2025-03-21T09:13:08.471707Z"},"id":"Ima9ua0FQnPv","outputId":"a5cba302-747d-438b-a6c0-5ce7e1a0a6e8","papermill":{"duration":0.061328,"end_time":"2025-03-21T09:13:08.473588","exception":false,"start_time":"2025-03-21T09:13:08.41226","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## defining X (features) and y (prediction)\nX = data_new.drop(columns=['emotions'])\ny = data_new['emotions'].to_numpy()","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:13:08.555375Z","iopub.status.busy":"2025-03-21T09:13:08.554992Z","iopub.status.idle":"2025-03-21T09:13:08.559768Z","shell.execute_reply":"2025-03-21T09:13:08.559136Z"},"papermill":{"duration":0.046691,"end_time":"2025-03-21T09:13:08.561003","exception":false,"start_time":"2025-03-21T09:13:08.514312","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## we need to encode y as it contains 8 classes, which are object, hence need to use OneHotEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder()\ny_encoded = encoder.fit_transform(y.reshape(-1, 1))\n\n## how it is encoded\nencoder.get_feature_names_out()","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:13:08.643592Z","iopub.status.busy":"2025-03-21T09:13:08.643181Z","iopub.status.idle":"2025-03-21T09:13:08.65107Z","shell.execute_reply":"2025-03-21T09:13:08.650323Z"},"papermill":{"duration":0.051281,"end_time":"2025-03-21T09:13:08.652362","exception":false,"start_time":"2025-03-21T09:13:08.601081","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## lets check shape of y_encoded\nprint(f\"Shape of y_encoded : {y_encoded.shape}\")","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:13:08.735388Z","iopub.status.busy":"2025-03-21T09:13:08.734977Z","iopub.status.idle":"2025-03-21T09:13:08.739084Z","shell.execute_reply":"2025-03-21T09:13:08.738157Z"},"papermill":{"duration":0.047271,"end_time":"2025-03-21T09:13:08.740286","exception":false,"start_time":"2025-03-21T09:13:08.693015","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_encoded = y_encoded.toarray()\ny_encoded","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:13:08.821446Z","iopub.status.busy":"2025-03-21T09:13:08.821197Z","iopub.status.idle":"2025-03-21T09:13:08.828378Z","shell.execute_reply":"2025-03-21T09:13:08.827726Z"},"papermill":{"duration":0.049068,"end_time":"2025-03-21T09:13:08.829654","exception":false,"start_time":"2025-03-21T09:13:08.780586","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## lets split it into training and testing set\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, shuffle=True, random_state=42)\n\n\n\nprint(f\"Shape of X_train : {X_train.shape}\")\nprint(f\"Shape of X_test : {X_test.shape}\")\nprint(f\"Shape of y_train : {y_train.shape}\")\nprint(f\"Shape of y_test : {y_test.shape}\")","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:13:08.91354Z","iopub.status.busy":"2025-03-21T09:13:08.913288Z","iopub.status.idle":"2025-03-21T09:13:08.922393Z","shell.execute_reply":"2025-03-21T09:13:08.921708Z"},"papermill":{"duration":0.053808,"end_time":"2025-03-21T09:13:08.923978","exception":false,"start_time":"2025-03-21T09:13:08.87017","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## scaling features\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nprint(X_train.shape)\nprint(X_test.shape)","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:13:09.046016Z","iopub.status.busy":"2025-03-21T09:13:09.045506Z","iopub.status.idle":"2025-03-21T09:13:09.06854Z","shell.execute_reply":"2025-03-21T09:13:09.067651Z"},"papermill":{"duration":0.066352,"end_time":"2025-03-21T09:13:09.069905","exception":false,"start_time":"2025-03-21T09:13:09.003553","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, LSTM, Dense, Dropout, TimeDistributed, Bidirectional\nfrom tensorflow.keras.optimizers import Adam","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:13:09.151982Z","iopub.status.busy":"2025-03-21T09:13:09.151677Z","iopub.status.idle":"2025-03-21T09:13:09.155446Z","shell.execute_reply":"2025-03-21T09:13:09.154601Z"},"papermill":{"duration":0.046074,"end_time":"2025-03-21T09:13:09.156613","exception":false,"start_time":"2025-03-21T09:13:09.110539","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\nX_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:13:09.240358Z","iopub.status.busy":"2025-03-21T09:13:09.240103Z","iopub.status.idle":"2025-03-21T09:13:09.243925Z","shell.execute_reply":"2025-03-21T09:13:09.243042Z"},"papermill":{"duration":0.047795,"end_time":"2025-03-21T09:13:09.245259","exception":false,"start_time":"2025-03-21T09:13:09.197464","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install tensorflow-addons\n","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:13:09.328275Z","iopub.status.busy":"2025-03-21T09:13:09.328041Z","iopub.status.idle":"2025-03-21T09:13:13.274524Z","shell.execute_reply":"2025-03-21T09:13:13.273419Z"},"papermill":{"duration":3.989468,"end_time":"2025-03-21T09:13:13.276159","exception":false,"start_time":"2025-03-21T09:13:09.286691","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CNN-A-BILSTM MODEL","metadata":{"papermill":{"duration":0.047079,"end_time":"2025-03-21T09:13:13.366451","exception":false,"start_time":"2025-03-21T09:13:13.319372","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from tensorflow.keras.layers import Lambda\nmodel = Sequential()\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPooling2D, Dropout, Flatten, Dense, Bidirectional, LSTM, Lambda\nfrom tensorflow.keras.optimizers import RMSprop\n\n# Define Attention Layer\nclass AttentionLayer(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(AttentionLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1],), initializer=\"normal\", trainable=True)\n        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[-2],), initializer=\"zeros\", trainable=True)\n        super(AttentionLayer, self).build(input_shape)\n\n    def call(self, x):\n        e = tf.nn.tanh(tf.tensordot(x, self.W, axes=1) + self.b)\n        a = tf.nn.softmax(e, axis=1)\n        return tf.reduce_sum(x * tf.expand_dims(a, -1), axis=1)\n\n# Define Model\ninput_layer = Input(shape=(X_train.shape[1], X_train.shape[2], 1))\n\n# **Efficient CNN Feature Extractor**\nx = Conv2D(64, (5, 5), padding='same', activation='relu')(input_layer)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size=(1, 2), padding='same')(x)\nx = Dropout(0.2)(x)  # Reduced dropout slightly\n\nx = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size=(1, 2), padding='same')(x)\nx = Dropout(0.3)(x)  # More aggressive dropout here\n\n# **Flatten and Transition to LSTM**\nx = Flatten()(x)\nx = Dense(256, activation='relu')(x)\nx = Dropout(0.3)(x)\n\n# **Stronger LSTM Layers Instead of Deep CNN**\nx = Lambda(lambda t: tf.expand_dims(t, axis=1))(x)\nx = Bidirectional(LSTM(256, return_sequences=True))(x)  # Increased LSTM units\nx = Bidirectional(LSTM(128, return_sequences=True))(x)  # Second BiLSTM layer\n\n# **Attention Layer for Focus**\nx = AttentionLayer()(x)\n\n# **Final Dense Layers**\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.3)(x)\nout = Dense(y_train.shape[1], activation='softmax')(x)\n\n# **Compile Model with RMSprop**\nmodel = Model(inputs=input_layer, outputs=out)\nmodel.compile(loss='categorical_crossentropy', optimizer=RMSprop(learning_rate=0.0005), metrics=['accuracy'])\n","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:13:13.449984Z","iopub.status.busy":"2025-03-21T09:13:13.449637Z","iopub.status.idle":"2025-03-21T09:13:16.21955Z","shell.execute_reply":"2025-03-21T09:13:16.218568Z"},"papermill":{"duration":2.813533,"end_time":"2025-03-21T09:13:16.221167","exception":false,"start_time":"2025-03-21T09:13:13.407634","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:13:16.305573Z","iopub.status.busy":"2025-03-21T09:13:16.305296Z","iopub.status.idle":"2025-03-21T09:13:16.330884Z","shell.execute_reply":"2025-03-21T09:13:16.33018Z"},"papermill":{"duration":0.068449,"end_time":"2025-03-21T09:13:16.332084","exception":false,"start_time":"2025-03-21T09:13:16.263635","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\n# Early stopping to monitor validation loss\nearly_stopping = EarlyStopping(\n    monitor='val_loss',\n    patience=10,  # Stop training if val_loss doesn't improve for 10 epochs\n    restore_best_weights=True\n)","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:13:16.418393Z","iopub.status.busy":"2025-03-21T09:13:16.418137Z","iopub.status.idle":"2025-03-21T09:13:16.423936Z","shell.execute_reply":"2025-03-21T09:13:16.423284Z"},"papermill":{"duration":0.05119,"end_time":"2025-03-21T09:13:16.42535","exception":false,"start_time":"2025-03-21T09:13:16.37416","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.losses import CategoricalCrossentropy\n\n# Focal loss function\ndef focal_loss(gamma=2.0, alpha=0.25):\n    def loss_fn(y_true, y_pred):\n        cce = CategoricalCrossentropy()\n        cross_entropy = cce(y_true, y_pred)\n        pt = tf.exp(-cross_entropy)\n        focal_loss = alpha * (1 - pt) ** gamma * cross_entropy\n        return focal_loss\n    return loss_fn\n","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:13:16.510448Z","iopub.status.busy":"2025-03-21T09:13:16.510185Z","iopub.status.idle":"2025-03-21T09:13:16.51496Z","shell.execute_reply":"2025-03-21T09:13:16.514301Z"},"papermill":{"duration":0.048645,"end_time":"2025-03-21T09:13:16.516179","exception":false,"start_time":"2025-03-21T09:13:16.467534","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Learning Rate Scheduling\nlr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n\ndef clr_schedule(epoch, lr):\n    min_lr = 1e-5\n    max_lr = 5e-4\n    cycle = 15\n    return min_lr + (max_lr - min_lr) * abs((epoch % (2 * cycle)) - cycle) / cycle\n\nclr_callback = LearningRateScheduler(clr_schedule)\n","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:13:16.602841Z","iopub.status.busy":"2025-03-21T09:13:16.60255Z","iopub.status.idle":"2025-03-21T09:13:16.606895Z","shell.execute_reply":"2025-03-21T09:13:16.606246Z"},"papermill":{"duration":0.049481,"end_time":"2025-03-21T09:13:16.608181","exception":false,"start_time":"2025-03-21T09:13:16.5587","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras import metrics\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss=focal_loss(), metrics=['accuracy', metrics.Precision(), metrics.Recall()])","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:13:16.693806Z","iopub.status.busy":"2025-03-21T09:13:16.693502Z","iopub.status.idle":"2025-03-21T09:13:16.707855Z","shell.execute_reply":"2025-03-21T09:13:16.707229Z"},"papermill":{"duration":0.058463,"end_time":"2025-03-21T09:13:16.70906","exception":false,"start_time":"2025-03-21T09:13:16.650597","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=128, callbacks=[lr_schedule])\n","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:13:16.79448Z","iopub.status.busy":"2025-03-21T09:13:16.794241Z","iopub.status.idle":"2025-03-21T09:14:24.742037Z","shell.execute_reply":"2025-03-21T09:14:24.741314Z"},"papermill":{"duration":67.9925,"end_time":"2025-03-21T09:14:24.743377","exception":false,"start_time":"2025-03-21T09:13:16.750877","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_loss, test_accuracy, test_precision, test_recall = model.evaluate(X_test, y_test, verbose=1)\nprint(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Test Precision: {test_precision:.4f}\")\nprint(f\"Test Recall: {test_recall:.4f}\")","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:14:24.936273Z","iopub.status.busy":"2025-03-21T09:14:24.935966Z","iopub.status.idle":"2025-03-21T09:14:25.21089Z","shell.execute_reply":"2025-03-21T09:14:25.210127Z"},"papermill":{"duration":0.371717,"end_time":"2025-03-21T09:14:25.212301","exception":false,"start_time":"2025-03-21T09:14:24.840584","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Plot accuracy\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.legend()\nplt.title('Accuracy')","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:14:25.402173Z","iopub.status.busy":"2025-03-21T09:14:25.401637Z","iopub.status.idle":"2025-03-21T09:14:25.60408Z","shell.execute_reply":"2025-03-21T09:14:25.603197Z"},"papermill":{"duration":0.298258,"end_time":"2025-03-21T09:14:25.605475","exception":false,"start_time":"2025-03-21T09:14:25.307217","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.legend()\nplt.title('Loss')\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:14:25.813316Z","iopub.status.busy":"2025-03-21T09:14:25.812992Z","iopub.status.idle":"2025-03-21T09:14:25.982613Z","shell.execute_reply":"2025-03-21T09:14:25.98176Z"},"papermill":{"duration":0.274339,"end_time":"2025-03-21T09:14:25.98398","exception":false,"start_time":"2025-03-21T09:14:25.709641","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\n\n# Get model predictions on the test set\n# Evaluate Model\ny_pred = model.predict(X_test)\ny_pred_labels = np.argmax(y_pred, axis=1)\ny_true_labels = np.argmax(y_test, axis=1)\naccuracy = accuracy_score(y_true_labels, y_pred_labels)\nprint(f'Model Accuracy: {accuracy * 100:.2f}%')\n\n# Compute Precision, Recall, F1-Score\nprint(\"Classification Report:\\n\", classification_report(y_true_labels, y_pred_labels, digits=4))\n","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:14:26.182225Z","iopub.status.busy":"2025-03-21T09:14:26.181926Z","iopub.status.idle":"2025-03-21T09:14:26.690242Z","shell.execute_reply":"2025-03-21T09:14:26.689351Z"},"papermill":{"duration":0.606724,"end_time":"2025-03-21T09:14:26.691592","exception":false,"start_time":"2025-03-21T09:14:26.084868","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\n# Compute Confusion Matrix\nconf_matrix = confusion_matrix(y_true_labels, y_pred_labels)\n\n# Normalize Confusion Matrix (Row-wise Normalization)\nconf_matrix_norm = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n\n# Plot Confusion Matrix with Accuracy\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix_norm, annot=True, fmt=\".2f\", cmap=\"Blues\", xticklabels=['disgust', 'anger', 'fearful', 'sad', 'happy', 'neutral', 'pleasant_surprise'], yticklabels=['disgust', 'anger', 'fearful', 'sad', 'happy', 'neutral', 'pleasant_surprise'])\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Normalized Confusion Matrix (Accuracy per Class)\")\nplt.show()\n","metadata":{"execution":{"iopub.execute_input":"2025-03-21T09:14:26.886608Z","iopub.status.busy":"2025-03-21T09:14:26.886314Z","iopub.status.idle":"2025-03-21T09:14:27.205693Z","shell.execute_reply":"2025-03-21T09:14:27.204818Z"},"papermill":{"duration":0.419137,"end_time":"2025-03-21T09:14:27.207287","exception":false,"start_time":"2025-03-21T09:14:26.78815","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.098166,"end_time":"2025-03-21T09:14:27.402935","exception":false,"start_time":"2025-03-21T09:14:27.304769","status":"completed"},"tags":[]},"outputs":[],"execution_count":null}]}